\subsection{Random Forest}
Finally, we applied the random forest classifier as our sixth classification algorithm. In
general, random forests are used in classification and regression problems~\cite{ref_rapidminerRandomForest}.
A random forest is an ensemble method which consists of a collection of
multiple random decision trees. In many data sets, the random forest performs better than the decision tree classifiers ~\cite{ref_Tan}. Each tree is generated by random vectors of the training data sets. The nodes in the decision
trees are represented by the attributes. \newline 
By applying the model to new examples, each tree predicts a class by following the branches of the decision tree. The output is a voting classification model, that includes the decision trees in the random forest. Which means, that each tree in this forest makes a decision and the class with the most votes determines the final class. The classification of the random forest varies less than the individual classification of each random tree. This is due to the fact that every classification in the random forest is treated equally important ~\cite{ref_rapidminerRandomForest}. \newline
To find the optimal values the ``Optimize Parameter'' operator ran with different values for number of trees (20 to 90 in steps of 7 with linear scale) and maximal depth (0 to 100 in steps of 10 with linear scale), using the accuracy as splitting criterion. The resulting
optimal values for number of trees is 90 and for maximal depth is 50 (as well as: 76 for number of trees and 100 for maximal depth and 90 for number of trees and 50 for maximal depth). 
For the first combination of values the following results were scored: $accuracy = 88.39\%$, $weighted precision = 90.22\%$ , and $weighted recall = 89.15\%$. Our best performing class was again the goalkeeper with a precision of 100\% and our worst performing class was again the midfielder with a precision of 83.53\%. The model was tested for the three
value combinations mentioned above. 
Besides accuracy also gain ratio, information gain and gini index where used as splitting criterion. 
The best result was archived with 76 trees and a maximal depth of 100, using information gain as splitting criterion. 
The results for this value combination are shown in table \ref{tab:RandomForest}.
\begin{table}[h]
\centering
\begin{tabular}{@{}llll@{}}
\toprule
Splitting criterion                            & Accuracy         & Weighted Recall  & Weighted Precision \\ \midrule
\multicolumn{1}{l|}{Accuracy}                  & 84,63\%          & 85,77\%          & 87,13\%            \\
\multicolumn{1}{l|}{Gain Ratio}                & 87,55\%          & 87,67\%          & 89,24\%            \\
\multicolumn{1}{l|}{\textbf{Information Gain}} & \textbf{88,67\%} & \textbf{89,25\%} & \textbf{89,81\%}   \\
\multicolumn{1}{l|}{GINI index}                & 88,62\%          & 89,23\%          & 89,70\%            \\ \bottomrule
\end{tabular}
\label{tab:RandomForest}
\caption{Results of random forest model on test data}
\end{table}