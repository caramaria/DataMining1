\subsection{Random Forest}

At the end, we applied the random forest classifier as our sixth classification algorithm. In
general, Random Forests are used in classification and regression problems~\cite{ref_rapidminerRandomForest}.
A random forest is an ensemble method which consists of a collection of
multiple random decision trees. In many data sets, the Random Forest perform better than
decision tree classifiers ~\cite{ref_Tan}. Each tree is generated by random vectors of the training data sets. The nodes in the decision
trees are represented by the attributes. \newline 
When applying the model to new examples, each tree predicts a class by following the branches of the decision tree. The output is a voting
classification model which combines the decision trees in the random forest which means that
each tree in this forest make a decision and the class with the most votes determines the final
class. The classification of the random forest varies less than of each random tree on its own,
because every classification in the random forest is treated equally important. ~\cite{ref_rapidminerRandomForest} \newline
To find the optimal
values Optimize Parameter operator ran with different
values for number of trees (20 to 90 in steps of 7 with linear scale) and maximal depth (0 to
100 in steps of 10 with linear scale). Using the accuracy as splitting criterion. The resulting
optimal values for number of trees is 90 and for maximal depth is 50 (as well as: 76 for
number of trees and 100 for maximal depth and 90 for number of trees and 50 for maximal
depth). For the first combination of values, an accuracy of 88.39\% was scored, weighted precision of
90.22\% and a weighted recall of 89.15\%. Our best performing class was again the goalkeeper with a
precision of 100\% and our worst performing class was again the midfielder with a precision
of 83.53\%. The model was tested with FIFA17 data for the three
value combinations mentioned above. Different splitting criterions beside
Accuracy like Gain ratio, Information gain and Gini index were tried. The best result was archieved with
76 trees and a maximal depth of 100, using Information Gain as splitting criterion. The
results for this value combination are shown in table \ref{tab:RandomForest}.

\begin{table}[]
\begin{tabular}{@{}llll@{}}
\toprule
Splitting criterion                            & Accuracy         & Weighted Recall  & Weighted Precision \\ \midrule
\multicolumn{1}{l|}{Accuracy}                  & 84,63\%          & 85,77\%          & 87,13\%            \\
\multicolumn{1}{l|}{Gain Ratio}                & 87,55\%          & 87,67\%          & 89,24\%            \\
\multicolumn{1}{l|}{\textbf{Information Gain}} & \textbf{88,67\%} & \textbf{89,25\%} & \textbf{89,81\%}   \\
\multicolumn{1}{l|}{GINI index}                & 88,62\%          & 89,23\%          & 89,70\%            \\ \bottomrule
\end{tabular}
\label{tab:RandomForest}
\caption{Results of random forest model on test data}
\end{table}